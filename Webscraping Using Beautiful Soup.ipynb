{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping for SEO Content Analysis\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project demonstrates a systematic approach to web scraping for educational content analysis. By extracting structured data from an SEO tutorial website, this notebook showcases key data engineering skills including HTTP requests, HTML parsing, and data transformation.\n",
        "\n",
        "**Technologies:** Python, Requests, BeautifulSoup, Pandas  \n",
        "**Domain:** Digital Marketing Analytics | Content Strategy\n",
        "\n",
        "---\n",
        "\n",
        "## Business Context\n",
        "\n",
        "Understanding the structure and organization of educational content can help businesses:\n",
        "- **Content Strategy:** Identify topic coverage and gaps in competitor educational resources\n",
        "- **SEO Analysis:** Understand how tutorial sites organize information for search optimization\n",
        "- **Learning Path Design:** Extract curricula structures for course development\n",
        "- **Market Research:** Analyze what topics are prioritized in digital marketing education\n",
        "\n",
        "---\n",
        "\n",
        "## Project Objectives\n",
        "\n",
        "1. **Extract** structured content from a web-based SEO tutorial\n",
        "2. **Parse** HTML to identify headings, links, and hierarchical structure\n",
        "3. **Transform** raw HTML into analyzable data formats\n",
        "4. **Demonstrate** production-ready web scraping techniques with error handling\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2B8jgvRnViRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup & Library Imports\n",
        "\n",
        "This section imports the essential Python libraries needed for web scraping and data manipulation:\n",
        "\n",
        "- **`requests`**: HTTP library for sending GET/POST requests to web servers and retrieving HTML content\n",
        "- **`BeautifulSoup (bs4)`**: HTML/XML parser that creates a navigable tree structure for extracting data from markup\n",
        "- **`pandas`**: Data manipulation library for structuring scraped data into DataFrames for analysis\n",
        "\n",
        "**Why these libraries?**  \n",
        "This minimal tech stack is industry-standard for web scraping projects, balancing simplicity with powerful functionality for most scraping use cases."
      ],
      "metadata": {
        "id": "STwsjH12V65i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymmuOfjbBK3-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Collection: HTTP Request & Page Retrieval\n",
        "\n",
        "This section demonstrates the fundamental web scraping workflow:\n",
        "\n",
        "### Code Breakdown:\n",
        "```python\n",
        "url = requests.get(\"https://www.tutorialsfreak.com/seo-tutorial\")\n",
        "```\n",
        "- Sends an HTTP GET request to the target URL\n",
        "- Returns a Response object containing status code, headers, and content\n",
        "- The `url` variable stores the entire response\n",
        "\n",
        "```python\n",
        "print(url)          # Shows Response object and status code\n",
        "print(url.text)     # Extracts raw HTML as string\n",
        "```\n",
        "\n",
        "### What This Accomplishes:\n",
        "✅ Validates server connectivity (status 200 = success)  \n",
        "✅ Retrieves full HTML source code of the page  \n",
        "✅ Provides raw data for subsequent parsing\n",
        "\n",
        "**Production Note:** In real-world applications, add error handling for timeouts, 404s, and rate limiting."
      ],
      "metadata": {
        "id": "WYlB1A7YWTkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. HTML Parsing & Data Structuring\n",
        "\n",
        "Now that we have the raw HTML, we'll use BeautifulSoup to parse it and extract structured data into a pandas DataFrame.\n",
        "\n",
        "### Objective:\n",
        "Extract SEO tutorial sections (topics, titles, and URLs) and organize them into a clean, analyzable dataset.\n",
        "\n",
        "### What We'll Extract:\n",
        "- **Section Titles**: Main topic headings (e.g., \"Introduction to SEO\", \"Keyword Research\")\n",
        "- **Article Titles**: Individual tutorial page names\n",
        "- **URLs**: Direct links to each tutorial page\n",
        "\n",
        "This structured dataset can then be used for content analysis, competitive benchmarking, or building a knowledge base index."
      ],
      "metadata": {
        "id": "uDOdVR5Cb4R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Parse the HTML with BeautifulSoup\n",
        "soup = BeautifulSoup(url.text, 'html.parser')\n",
        "\n",
        "# Step 2: Find all links on the page\n",
        "links = soup.find_all('a', href=True)\n",
        "\n",
        "# Step 3: Extract tutorial links (filter for SEO-related URLs)\n",
        "tutorial_data = []\n",
        "\n",
        "for link in links:\n",
        "    href = link.get('href')\n",
        "    title = link.get_text(strip=True)\n",
        "\n",
        "    # Filter for relevant tutorial links\n",
        "    if '/seo-tutorial/' in href and title:\n",
        "        tutorial_data.append({\n",
        "            'Title': title,\n",
        "            'URL': 'https://www.tutorialsfreak.com' + href if href.startswith('/') else href\n",
        "        })\n",
        "\n",
        "print(f\"Found {len(tutorial_data)} tutorial pages\")\n",
        "print(\"\\nFirst 5 tutorials:\")\n",
        "for i, item in enumerate(tutorial_data[:5], 1):\n",
        "    print(f\"{i}. {item['Title']}\")"
      ],
      "metadata": {
        "id": "39fs_obDcobC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Convert to pandas DataFrame\n",
        "df_tutorials = pd.DataFrame(tutorial_data)\n",
        "\n",
        "# Step 5: Add additional metadata\n",
        "df_tutorials['Scraped_Date'] = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
        "df_tutorials['Source'] = 'TutorialsFreak SEO Tutorial'\n",
        "\n",
        "# Step 6: Display dataset info\n",
        "print(\"=\"*60)\n",
        "print(\"STRUCTURED SEO TUTORIAL DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal Records: {len(df_tutorials)}\")\n",
        "print(f\"Columns: {list(df_tutorials.columns)}\")\n",
        "print(f\"\\nDataset Shape: {df_tutorials.shape}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FIRST 10 ROWS:\")\n",
        "print(\"=\"*60)\n",
        "df_tutorials.head(10)"
      ],
      "metadata": {
        "id": "Df32YBKcdSFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "15UAijBLd2Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Export to CSV for reusability\n",
        "filename = 'seo_tutorial_dataset.csv'\n",
        "df_tutorials.to_csv(filename, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ DATASET SUCCESSFULLY SAVED!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nFilename: {filename}\")\n",
        "print(f\"Location: Current working directory\")\n",
        "print(f\"Format: CSV (comma-separated values)\")\n",
        "print(f\"Records exported: {len(df_tutorials)}\")\n",
        "print(f\"\\nThis dataset can now be used for:\")\n",
        "print(\"  • Content gap analysis\")\n",
        "print(\"  • Competitive benchmarking\")\n",
        "print(\"  • SEO topic research\")\n",
        "print(\"  • Learning path design\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "id": "7v38tJe7d_IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Analysis & NLP: Keyword Extraction\n",
        "\n",
        "Now we'll perform basic Natural Language Processing (NLP) to extract insights from the scraped content.\n",
        "\n",
        "### Objective:\n",
        "Analyze the text content to identify the most emphasized SEO concepts and keywords on the tutorial page.\n",
        "\n",
        "### NLP Techniques Applied:\n",
        "- **Text Extraction**: Pull clean text from HTML body\n",
        "- **Text Normalization**: Convert to lowercase, remove punctuation and stopwords\n",
        "- **Tokenization**: Split text into individual words\n",
        "- **Frequency Analysis**: Count word occurrences to identify key themes\n",
        "\n",
        "### Business Value:\n",
        "Understanding keyword frequency helps with:\n",
        "- **Content Strategy**: Identify which SEO topics are most emphasized\n",
        "- **Competitive Analysis**: See what concepts competitors prioritize\n",
        "- **SEO Research**: Discover trending terminology in the industry"
      ],
      "metadata": {
        "id": "hDa51y6YgD78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Extract text from HTML body\n",
        "text_content = soup.body.get_text(separator=' ', strip=True)\n",
        "\n",
        "# Step 2: Clean the text - remove extra whitespace and normalize\n",
        "text_content = re.sub(r'\\s+', ' ', text_content)  # Replace multiple spaces with single space\n",
        "text_content = text_content.lower()  # Convert to lowercase\n",
        "\n",
        "# Step 3: Remove punctuation and split into words\n",
        "words = re.findall(r'\\b[a-z]+\\b', text_content)\n",
        "\n",
        "# Step 4: Define common stopwords to exclude\n",
        "stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
        "             'of', 'with', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
        "             'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should',\n",
        "             'can', 'could', 'may', 'might', 'must', 'this', 'that', 'these', 'those',\n",
        "             'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'which', 'who',\n",
        "             'when', 'where', 'why', 'how', 'all', 'each', 'every', 'both', 'few',\n",
        "             'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',\n",
        "             'own', 'same', 'so', 'than', 'too', 'very', 'as', 'from', 'by', 'up', 'about'}\n",
        "\n",
        "# Step 5: Filter out stopwords\n",
        "filtered_words = [word for word in words if word not in stopwords and len(word) > 2]\n",
        "\n",
        "print(f\"Total words extracted: {len(words):,}\")\n",
        "print(f\"Words after cleaning: {len(filtered_words):,}\")\n",
        "print(f\"\\nFirst 50 words: {' '.join(filtered_words[:50])}...\")"
      ],
      "metadata": {
        "id": "nNGps2Bag5pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Perform frequency analysis\n",
        "word_freq = Counter(filtered_words)\n",
        "top_keywords = word_freq.most_common(20)\n",
        "\n",
        "# Display results\n",
        "print(\"=\"*70)\n",
        "print(\"TOP 20 KEYWORDS IN SEO TUTORIAL CONTENT\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Rank':<6} {'Keyword':<20} {'Frequency':<12} {'Percentage'}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for rank, (word, count) in enumerate(top_keywords, 1):\n",
        "    percentage = (count / len(filtered_words)) * 100\n",
        "    print(f\"{rank:<6} {word:<20} {count:<12} {percentage:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ INSIGHTS:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Most emphasized keyword: '{top_keywords[0][0]}' (appears {top_keywords[0][1]} times)\")\n",
        "print(f\"\\nTop 5 SEO concepts: {', '.join([word for word, _ in top_keywords[:5]])}\")\n",
        "print(f\"\\nThese keywords represent the core topics covered in the tutorial,\")\n",
        "print(f\"helping identify content focus areas and competitive positioning.\")"
      ],
      "metadata": {
        "id": "XQqwltTXhUZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Business Applications & Insights\n",
        "\n",
        "### Potential Use Cases for This Scraping Framework:\n",
        "\n",
        "**1. Competitive Content Analysis**\n",
        "- Track how competitors structure their educational content\n",
        "- Identify topic gaps in your own content strategy\n",
        "- Benchmark against industry-standard SEO tutorial organization\n",
        "\n",
        "**2. Automated Content Auditing**\n",
        "- Periodically scrape tutorial sites to monitor for content updates\n",
        "- Alert teams when new topics are added to industry knowledge bases\n",
        "- Build a content coverage matrix for internal L&D teams\n",
        "\n",
        "**3. Lead Generation & Market Research**\n",
        "- Extract contact information and social links from tutorial pages\n",
        "- Build lists of educational resource providers for partnership outreach\n",
        "- Identify trending topics based on content volume and recency\n",
        "\n",
        "**4. SEO Research & Keyword Mapping**\n",
        "- Automatically extract meta tags, headings, and keyword density from tutorial pages\n",
        "- Compare on-page optimization strategies across multiple educational sites\n",
        "- Build a database of how top sites structure content around specific keywords\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sTc-9gCkWy1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion & Next Steps\n",
        "\n",
        "### What This Project Demonstrates:\n",
        "\n",
        "✅ **Technical Proficiency**: Ability to send HTTP requests, parse HTML, and extract structured data  \n",
        "✅ **Business Acumen**: Understanding how web scraping supports content strategy, SEO research, and competitive intelligence  \n",
        "✅ **Documentation Standards**: Clear, recruiter-friendly explanations of technical concepts  \n",
        "✅ **Production Awareness**: Recognition of error handling, rate limiting, and ethical scraping practices\n",
        "\n",
        "---\n",
        "\n",
        "### Recommended Enhancements:\n",
        "\n",
        "To make this project even stronger for your portfolio:\n",
        "\n",
        "1. **Add Data Transformation**: Use BeautifulSoup to extract specific elements (headings, links, meta tags) into a pandas DataFrame\n",
        "2. **Include Basic NLP**: Word frequency analysis or keyword extraction from the scraped text\n",
        "3. **Create Visualizations**: Bar chart showing distribution of topics or keyword frequency\n",
        "4. **Scale to Multiple Pages**: Loop through multiple tutorial URLs and aggregate data\n",
        "5. **Add Error Handling**: try/except blocks, status code checks, and timeout management\n",
        "6. **Export Results**: Save final DataFrame to CSV for reusability\n"
      ],
      "metadata": {
        "id": "IlOBcuewXNE3"
      }
    }
  ]
}